{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, re, os, json\n",
    "sys.path.append(\"..\")\n",
    "from rxnutils import read_json, is_valid_smiles\n",
    "MODEL_NAME = \"ether0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(response: str):\n",
    "    # Attempt to extract a JSON code block and parse it\n",
    "    # str format: \"<|think_start|>...<|think_end|><|answer_start|>...<|answer_end|>\"\n",
    "    # we extract the answer part\n",
    "    answer_match = re.search(r'<\\|answer_start\\|>(.*?)<\\|answer_end\\|>', response, re.DOTALL)\n",
    "    if answer_match:\n",
    "        answer = answer_match.group(1).strip()\n",
    "        return answer\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_RCR(model_name:str, log_dir:str=\"logs/RCR\"):\n",
    "    \"\"\"\n",
    "    Evaluate the reaction condition recommendation task\n",
    "    Metric: SMILES similarity\n",
    "\n",
    "    Args:\n",
    "        model_name (str): the name of the model\n",
    "        log_dir (str): the directory of the logs\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # make sure the log_dir is correct\n",
    "    if not os.path.exists(log_dir):\n",
    "        raise ValueError(f\"logs_dir {log_dir} is not correct\")\n",
    "    samples = read_json(f\"{log_dir}/{model_name}.json\")\n",
    "    preds = []\n",
    "    gts = []\n",
    "    for sample in samples:\n",
    "        gts.append(sample['gt'])\n",
    "\n",
    "        pred_smiles = parse_response(sample['json_response'])\n",
    "        # further parsing to remove the answer tag\n",
    "        if pred_smiles.endswith(\"</answer>\"):\n",
    "            pred_smiles = pred_smiles[:-len(\"</answer>\")]\n",
    "        \n",
    "        # further parsing to extract condition smiles\n",
    "        if \">>\" in pred_smiles:\n",
    "            pred_smiles = pred_smiles.split(\">>\")[0]\n",
    "        elif '>' in pred_smiles:\n",
    "            pred_smiles = pred_smiles.split('>')[1]\n",
    "        preds.append(pred_smiles)\n",
    "\n",
    "    from evaluator import MoleculeSMILESEvaluator\n",
    "    evaluator = MoleculeSMILESEvaluator()\n",
    "    res = evaluator.evaluate(preds, gts)\n",
    "    # pretty print the res\n",
    "    print(\"exact_match: \", round(res['exact_match'], 2))\n",
    "    print(\"bleu: \", round(res['bleu'], 2))\n",
    "    print(\"levenshtein: \", round(res['levenshtein'], 2))\n",
    "    print(\"rdk_sims: \", round(res['rdk_sims'], 2))\n",
    "    print(\"maccs_sims: \", round(res['maccs_sims'], 2))\n",
    "    print(\"morgan_sims: \", round(res['morgan_sims'], 2))\n",
    "    print(\"validity: \", round(res['validity'], 2))\n",
    "    fts = (res['rdk_sims'] + res['maccs_sims'] + res['morgan_sims']) / 3\n",
    "    print(\"fts: \", round(fts * res['validity'], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_RCR(\"ether0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mechanism: NEPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_NEPP(model_name: str, log_dir: str):\n",
    "    \"\"\"\n",
    "    Evaluate the next element-step product prediction task\n",
    "    Metric: SMILES similarity\n",
    "\n",
    "    Args:\n",
    "        model_name (str): the name of the model\n",
    "        log_dir (str): the directory of the logs\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(log_dir):\n",
    "        raise ValueError(f\"logs_dir {log_dir} is not correct\")\n",
    "    samples = read_json(f\"{log_dir}/{model_name}.json\")\n",
    "    preds = []\n",
    "    gts = []\n",
    "    for sample in samples:\n",
    "        gts.append(sample['gt'])\n",
    "        json_response = sample['json_response']\n",
    "        pred_smiles = parse_response(json_response)\n",
    "        preds.append(pred_smiles)\n",
    "\n",
    "    from evaluator import MoleculeSMILESEvaluator\n",
    "    evaluator = MoleculeSMILESEvaluator()\n",
    "    res = evaluator.evaluate(preds, gts)\n",
    "\n",
    "    # pretty print the res\n",
    "    print(\"exact_match: \", round(res['exact_match'], 2))\n",
    "    print(\"bleu: \", round(res['bleu'], 2))\n",
    "    print(\"levenshtein: \", round(res['levenshtein'], 2))\n",
    "    print(\"rdk_sims: \", round(res['rdk_sims'], 2))\n",
    "    print(\"maccs_sims: \", round(res['maccs_sims'], 2))\n",
    "    print(\"morgan_sims: \", round(res['morgan_sims'], 2))\n",
    "    print(\"validity: \", round(res['validity'], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact_match:  0.02\n",
      "bleu:  0.52\n",
      "levenshtein:  24.98\n",
      "rdk_sims:  0.62\n",
      "maccs_sims:  0.61\n",
      "morgan_sims:  0.53\n",
      "validity:  0.91\n"
     ]
    }
   ],
   "source": [
    "evaluate_NEPP(\"ether0\", log_dir=\"logs/mech_task1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mechanism: MechSel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCQ Accuracy (mean): 0.27\n"
     ]
    }
   ],
   "source": [
    "def evaluate_mechsel(model_name: str, logs_dir: str):\n",
    "    \"\"\"\n",
    "    Evaluate the reaction mechanism selection prediction.\n",
    "\n",
    "    Args:\n",
    "        logs_dir (str): The directory where the logs are stored.\n",
    "        model_name (str): The name of the model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(logs_dir):\n",
    "        raise ValueError(f\"logs_dir {logs_dir} is not correct\")\n",
    "    samples = read_json(f\"{logs_dir}/{model_name}.json\")\n",
    "\n",
    "    preds = []\n",
    "    gts = []\n",
    "    for sample in samples:\n",
    "        pred_choice = parse_response(sample['json_response'])\n",
    "        if len(pred_choice) > 1:\n",
    "            # if multiple chars, we take the first one\n",
    "            pred_choice = pred_choice[0]\n",
    "        # if pred_choice is not a valid choice, we treat it as empty\n",
    "        if pred_choice.lower() not in ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']:\n",
    "            pred_choice = \"\"\n",
    "\n",
    "        pred_choice = pred_choice.lower()\n",
    "        gt = sample['gt'].lower()\n",
    "        preds.append(pred_choice)\n",
    "        gts.append(gt)\n",
    "\n",
    "    accuracy = sum(1 for pred, gt in zip(preds, gts) if pred == gt) / len(gts)\n",
    "    print(f\"MCQ Accuracy (mean): {accuracy:.2f}\")\n",
    "\n",
    "# Example usage\n",
    "evaluate_mechsel(MODEL_NAME, \"logs/mech_task2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## major product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact_match:  0.74\n",
      "bleu:  0.77\n",
      "levenshtein:  12.56\n",
      "rdk_sims:  0.83\n",
      "maccs_sims:  0.86\n",
      "morgan_sims:  0.82\n",
      "validity:  0.92\n"
     ]
    }
   ],
   "source": [
    "def evaluate_fs(model_name: str, log_dir: str):\n",
    "    \"\"\"\n",
    "    Evaluate SMILES predictions against ground truth.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the model.\n",
    "        log_dir (str): The directory where the logs are stored.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(log_dir):\n",
    "        raise ValueError(f\"logs_dir {log_dir} is not correct\")\n",
    "    samples = read_json(f\"{log_dir}/{model_name}.json\")\n",
    "    preds = []\n",
    "    gts = []\n",
    "    for sample in samples:\n",
    "        ## parse the gt\n",
    "        try:\n",
    "            gt = sample['products']\n",
    "        except:\n",
    "            gt = json.loads(sample['gt']).get(\"Major Product\")\n",
    "        if isinstance(gt, list):\n",
    "            gt = '.'.join(gt)\n",
    "        gts.append(gt)\n",
    "\n",
    "        ## parse the pred\n",
    "        pred_smiles = parse_response(sample['json_response'])\n",
    "        preds.append(pred_smiles)\n",
    "\n",
    "    from evaluator import MoleculeSMILESEvaluator\n",
    "    evaluator = MoleculeSMILESEvaluator()\n",
    "    res = evaluator.evaluate(preds, gts)\n",
    "\n",
    "    print(\"exact_match: \", round(res['exact_match'], 2))\n",
    "    print(\"bleu: \", round(res['bleu'], 2))\n",
    "    print(\"levenshtein: \", round(res['levenshtein'], 2))\n",
    "    print(\"rdk_sims: \", round(res['rdk_sims'], 2))\n",
    "    print(\"maccs_sims: \", round(res['maccs_sims'], 2))\n",
    "    print(\"morgan_sims: \", round(res['morgan_sims'], 2))\n",
    "    print(\"validity: \", round(res['validity'], 2))\n",
    "    fts = (res['rdk_sims'] + res['maccs_sims'] + res['morgan_sims']) / 3\n",
    "    print(\"fts: \", round(fts * res['validity'], 2))\n",
    "\n",
    "# Example usage\n",
    "evaluate_fs(MODEL_NAME, \"logs/fs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## by product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_raw_response(raw_response: str, field: str) -> str:\n",
    "    \"\"\"从原始响应字符串中提取指定字段的值\n",
    "    \n",
    "    Args:\n",
    "        raw_response: 可能包含 JSON 块或键值对的原始字符串\n",
    "        field: 需要提取的字段名（如 \"Byproduct(s)\"）\n",
    "    \n",
    "    Returns:\n",
    "        提取到的字段值字符串，未找到则返回空字符串\n",
    "    \"\"\"\n",
    "    \n",
    "    # 改进点 1: 更健壮的 JSON 块检测（支持不同格式的代码块）\n",
    "    json_block_match = re.search(\n",
    "        r'```(?:json)?\\s*({.*?})\\s*```',  # 匹配可选的 json 标记和任意空格\n",
    "        raw_response, \n",
    "        flags=re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    if json_block_match:\n",
    "        try:\n",
    "            # 改进点 2: 更严格的 JSON 解析（处理嵌套结构）\n",
    "            json_str = json_block_match.group(1).strip()\n",
    "            data = json.loads(json_str)\n",
    "            \n",
    "            # 改进点 3: 处理数组型结果（如 [\"HCl\", \"H2O\"]）\n",
    "            if value := data.get(field):\n",
    "                if isinstance(value, list):\n",
    "                    return \", \".join(map(str, value))\n",
    "                return str(value)\n",
    "        except (json.JSONDecodeError, AttributeError):\n",
    "            pass  # 解析失败则继续尝试其他方法\n",
    "\n",
    "    # 改进点 4: 更灵活的正则匹配（处理多种引号和转义）\n",
    "    escaped_field = re.escape(field)  # 转义特殊字符如括号\n",
    "    \n",
    "    # 模式 1: 匹配双引号字符串（允许转义双引号）\n",
    "    pattern = fr'\"{escaped_field}\":\\s*\"((?:\\\\\"|[^\"])*)\"'\n",
    "    if match := re.search(pattern, raw_response):\n",
    "        try:\n",
    "            # 使用 JSON 解析处理转义字符\n",
    "            return json.loads(f'\"{match.group(1)}\"')\n",
    "        except json.JSONDecodeError:\n",
    "            return match.group(1).replace(r'\\\"', '\"')\n",
    "    \n",
    "    # 模式 2: 匹配单引号字符串（允许转义单引号）\n",
    "    pattern = fr'\"{escaped_field}\":\\s*\\'((?:\\\\\\'|[^\\'])*)\\''\n",
    "    if match := re.search(pattern, raw_response):\n",
    "        return match.group(1).replace(r\"\\'\", \"'\")\n",
    "    \n",
    "    # 改进点 5: 匹配无引号的值（如数值或布尔值）\n",
    "    pattern = fr'\"{escaped_field}\":\\s*([^,\\s}}]+)'\n",
    "    if match := re.search(pattern, raw_response):\n",
    "        return match.group(1).strip()\n",
    "\n",
    "    return \"\"  # 所有模式均未匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact_match:  0.0\n",
      "bleu:  0.0\n",
      "levenshtein:  0.0\n",
      "rdk_sims:  0.0\n",
      "maccs_sims:  0.0\n",
      "morgan_sims:  0.0\n",
      "validity:  0.0\n"
     ]
    }
   ],
   "source": [
    "def evaluate_fs_byproduct(model_name, log_dir:str=\"logs/fs\"):\n",
    "    samples = read_json(f\"{log_dir}/{model_name}.json\")\n",
    "    preds = []\n",
    "    gts = []\n",
    "    \n",
    "    for sample in samples:\n",
    "        ## parse the gt\n",
    "        try:\n",
    "            gt = sample['byproducts']\n",
    "        except:\n",
    "            gt = json.loads(sample['gt'])[\"Byproduct(s)\"]\n",
    "        if len(gt) == 0:  # if no byproducts, skip the sample\n",
    "            continue\n",
    "        if isinstance(gt, list):\n",
    "            gt = '.'.join(gt)\n",
    "\n",
    "        ## parse the pred\n",
    "        if isinstance(sample['json_response'], dict):\n",
    "            if \"Byproduct(s)\" in sample['json_response']:\n",
    "                pred_smiles = sample['json_response'][\"Byproduct(s)\"]\n",
    "            else:\n",
    "                pred_smiles = parse_raw_response(sample['raw_response'], \"Byproduct(s)\")\n",
    "        else:\n",
    "            try:\n",
    "                pred_smiles = parse_raw_response(sample['raw_response'], \"Byproduct(s)\")\n",
    "            except:\n",
    "                pred_smiles = \"\"\n",
    "\n",
    "        gts.append(gt)\n",
    "        preds.append(pred_smiles)\n",
    "\n",
    "    from evaluator import MoleculeSMILESEvaluator\n",
    "    evaluator = MoleculeSMILESEvaluator()\n",
    "    res = evaluator.evaluate(preds, gts)\n",
    "    print(\"exact_match: \", round(res['exact_match'], 2))\n",
    "    print(\"bleu: \", round(res['bleu'], 2))\n",
    "    print(\"levenshtein: \", round(res['levenshtein'], 2))\n",
    "    print(\"rdk_sims: \", round(res['rdk_sims'], 2))\n",
    "    print(\"maccs_sims: \", round(res['maccs_sims'], 2))\n",
    "    print(\"morgan_sims: \", round(res['morgan_sims'], 2))\n",
    "    print(\"validity: \", round(res['validity'], 2))\n",
    "    fts = (res['rdk_sims'] + res['maccs_sims'] + res['morgan_sims']) / 3\n",
    "    print(\"fts: \", round(fts * res['validity'], 2))\n",
    "\n",
    "# Example usage\n",
    "evaluate_fs_byproduct(MODEL_NAME, \"logs/fs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact_match:  0.0\n",
      "bleu:  0.5\n",
      "levenshtein:  25.0\n",
      "rdk_sims:  0.51\n",
      "maccs_sims:  0.57\n",
      "morgan_sims:  0.43\n",
      "validity:  0.87\n",
      "fts:  0.44\n"
     ]
    }
   ],
   "source": [
    "def evaluate_retro(model_name: str, log_dir: str):\n",
    "    if not os.path.exists(log_dir):\n",
    "        raise ValueError(f\"logs_dir {log_dir} is not correct\")\n",
    "    samples = read_json(f\"{log_dir}/{model_name}.json\")\n",
    "    preds = []\n",
    "    gts = []\n",
    "\n",
    "    for sample in samples:\n",
    "        ## parse the gt\n",
    "        gt = sample['reactants']\n",
    "        if len(gt) == 0:\n",
    "            continue\n",
    "        if isinstance(gt, list):\n",
    "            gt = '.'.join(gt)\n",
    "        gts.append(gt)\n",
    "\n",
    "        ## parse the pred\n",
    "        pred_smiles = parse_response(sample['json_response'])\n",
    "        preds.append(pred_smiles)\n",
    "\n",
    "    from evaluator import MoleculeSMILESEvaluator\n",
    "    evaluator = MoleculeSMILESEvaluator()\n",
    "    res = evaluator.evaluate(preds, gts)\n",
    "\n",
    "    print(\"exact_match: \", round(res['exact_match'], 2))\n",
    "    print(\"bleu: \", round(res['bleu'], 2))\n",
    "    print(\"levenshtein: \", round(res['levenshtein'], 2))\n",
    "    print(\"rdk_sims: \", round(res['rdk_sims'], 2))\n",
    "    print(\"maccs_sims: \", round(res['maccs_sims'], 2))\n",
    "    print(\"morgan_sims: \", round(res['morgan_sims'], 2))\n",
    "    print(\"validity: \", round(res['validity'], 2))\n",
    "    fts = (res['rdk_sims'] + res['maccs_sims'] + res['morgan_sims']) / 3\n",
    "    print(\"fts: \", round(fts * res['validity'], 2))\n",
    "\n",
    "evaluate_retro(MODEL_NAME, \"logs/rs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mol-und"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg_samples:  0.6\n",
      "ring_count:  1.2\n",
      "eq_score:  0.45\n",
      "murcko:  0.16892521367521368\n",
      "ring_system:  0.0\n"
     ]
    }
   ],
   "source": [
    "task_dict = dict(\n",
    "    fg_samples=\"fg_samples\", \n",
    "    murcko='Murcko_scaffold', \n",
    "    ring_count='ring_count',\n",
    "    ring_system='ring_system_scaffold', \n",
    "    mutated='mutated_list', \n",
    "    permutated='permutated_list'\n",
    ")\n",
    "pred_key_dict = dict(\n",
    "    fg_samples=\"count\", murcko='Output Scaffold', ring_count='count',\n",
    "    ring_system='output', mutated='output', permutated='output'\n",
    ")\n",
    "gt_key_dict = dict(\n",
    "    fg_samples=\"fg_num\", murcko='largest_scaffold', ring_count='count',\n",
    "    ring_system='', mutated='', permutated=''\n",
    ")\n",
    "\n",
    "from rdkit import DataStructs, Chem\n",
    "from rdkit.Chem.Scaffolds.MurckoScaffold import GetScaffoldForMol, MurckoScaffoldSmiles # type: ignore\n",
    "from rdkit.Chem import rdFMCS, AllChem\n",
    "\n",
    "def scaffold_consistency(src_mol_list, tgt_mol_list)->tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the scaffold consistency before&after mol-opt, consistency includes: same or contain\n",
    "    Metric: Tanimoto molecule similarity\n",
    "\n",
    "    Args:\n",
    "        src_mol_list (list): list of source molecules\n",
    "        tgt_mol_list (list): list of target molecules\n",
    "\n",
    "    Returns:\n",
    "        tuple[float, float]: tuple of count of same scaffold and mean of scaffold similarity\n",
    "    \"\"\"\n",
    "    assert len(src_mol_list) == len(tgt_mol_list)\n",
    "\n",
    "    count_same = 0\n",
    "    scaffold_score = list()\n",
    "    for i in range(len(tgt_mol_list)):\n",
    "        src_smiles, tgt_smiles = src_mol_list[i], tgt_mol_list[i]\n",
    "        src_mol, tgt_mol = Chem.MolFromSmiles(src_smiles), Chem.MolFromSmiles(tgt_smiles)\n",
    "        if (src_mol == None or tgt_mol == None) or (src_smiles == \"\" or tgt_smiles == \"\"):\n",
    "            scaffold_score.append(0.0)\n",
    "            continue\n",
    "\n",
    "        murcko_scaffold_list = [\n",
    "            MurckoScaffoldSmiles(smiles) for smiles in [src_smiles, tgt_smiles]\n",
    "        ]\n",
    "\n",
    "        if len(set(murcko_scaffold_list)) == 1:\n",
    "            # if identical, score 1.0\n",
    "            scaffold_score.append(1.0)\n",
    "            count_same += 1\n",
    "        else:\n",
    "            ## Morgan Fingerprint for scaffold similarity\n",
    "            murcko_scaffold_mol_list = [\n",
    "                Chem.MolFromSmiles(murcko_scaffold_list[0]),\n",
    "                Chem.MolFromSmiles(murcko_scaffold_list[1]),\n",
    "            ]\n",
    "            mcs = rdFMCS.FindMCS(murcko_scaffold_mol_list)\n",
    "            mcs_mol = (\n",
    "                Chem.MolFromSmarts(mcs.smartsString) if mcs.numAtoms > 0 else None\n",
    "            )\n",
    "\n",
    "            if mcs_mol:\n",
    "                # 计算基于指纹的Tanimoto相似度\n",
    "                fp1 = AllChem.GetMorganFingerprintAsBitVect(\n",
    "                    murcko_scaffold_mol_list[0], 2, nBits=1024\n",
    "                )\n",
    "                fp2 = AllChem.GetMorganFingerprintAsBitVect(\n",
    "                    murcko_scaffold_mol_list[1], 2, nBits=1024\n",
    "                )\n",
    "                similarity = DataStructs.TanimotoSimilarity(fp1, fp2)\n",
    "            else:\n",
    "                similarity = 0.0\n",
    "\n",
    "            scaffold_score.append(similarity)\n",
    "\n",
    "    if len(tgt_mol_list) == 0:\n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    mean_scaffold_score = sum(scaffold_score) / len(scaffold_score)\n",
    "    return count_same, mean_scaffold_score\n",
    "\n",
    "\n",
    "def evaluate_fg_sample(model_name, log_dir=\"logs/fg_samples\"):\n",
    "    \"\"\"\n",
    "    Evaluate the function-group (fg) count task\n",
    "    Metric: MAE\n",
    "    \"\"\"\n",
    "    if not os.path.exists(log_dir):\n",
    "        raise ValueError(f\"logs_dir {log_dir} is not correct\")\n",
    "    gt_list = []\n",
    "    pred_list = []\n",
    "    pred_results = read_json(f\"{log_dir}/{model_name}.json\")\n",
    "    for pred in pred_results:\n",
    "        gt = pred[gt_key_dict['fg_samples']]\n",
    "        pred = parse_response(pred['json_results'])\n",
    "        ## Parsing ##\n",
    "        # if pred like \"{count:xx}\", we need to extract the count\n",
    "        if pred.startswith(\"{count:\"):\n",
    "            pred = pred.split(\":\")[1].split(\"}\")[0]\n",
    "        \n",
    "        # pred should be a number. if failed, we treat the pred as 0\n",
    "        if not pred.isdigit():\n",
    "            pred = 0\n",
    "        pred_list.append(pred)\n",
    "        gt_list.append(gt)\n",
    "    \n",
    "    assert len(gt_list) == len(pred_list), f\"len(gt_list): {len(gt_list)}, len(pred_list): {len(pred_list)}\"\n",
    "    score = sum([abs(int(pred_list[i])-int(gt_list[i])) for i in range(len(pred_list))]) / len(gt_list)\n",
    "    return score\n",
    "\n",
    "\n",
    "def evaluate_ring_count(model_name, log_dir=\"logs/ring_count\"):\n",
    "    \"\"\"\n",
    "    Evaluate the ring count task\n",
    "    Metric: MAE\n",
    "    \"\"\"\n",
    "    if not os.path.exists(log_dir):\n",
    "        raise ValueError(f\"logs_dir {log_dir} is not correct\")\n",
    "    gt_list = []\n",
    "    pred_list = []\n",
    "    pred_results = read_json(f\"{log_dir}/{model_name}.json\")\n",
    "    for pred in pred_results:\n",
    "        id_ = pred['id']\n",
    "        gt = pred[gt_key_dict['ring_count']]\n",
    "        pred = parse_response(pred['json_results'])\n",
    "        # if pred like \"{count:xx}\", we need to extract the count\n",
    "        if pred.startswith(\"{count:\"):\n",
    "            pred = pred.split(\":\")[1].split(\"}\")[0]\n",
    "        # pred should be a number. if failed, we treat the pred as 0\n",
    "        if not pred.isdigit():\n",
    "            pred = 0\n",
    "        pred_list.append(pred)\n",
    "        gt_list.append(gt)\n",
    "    \n",
    "    assert len(gt_list) == len(pred_list)\n",
    "    score = sum([abs(int(pred_list[i])-int(gt_list[i])) for i in range(len(pred_list))]) / len(pred_results)\n",
    "    return score\n",
    "\n",
    "\n",
    "def evaluate_mutate(model_name, log_dir=\"logs/mutated_list\"):\n",
    "    \"\"\"\n",
    "    Evaluate the mutated task\n",
    "    Metric: Accuracy\n",
    "    \"\"\"\n",
    "    if not os.path.exists(log_dir):\n",
    "        raise ValueError(f\"logs_dir {log_dir} is not correct\")\n",
    "    gt_list = []\n",
    "    pred_list = []\n",
    "    pred_results = read_json(f\"{log_dir}/{model_name}.json\")\n",
    "    for pred in pred_results:\n",
    "        gt = False\n",
    "        pred = parse_response(pred['json_results'])\n",
    "        pred = pred.lower()\n",
    "\n",
    "        # if failed to parse. we treat the pred as False and gt as True\n",
    "        # this is a special case for the model that cannot parse the response\n",
    "        if pred == \"\":\n",
    "            pred_list.append(False)\n",
    "            gt_list.append(True)\n",
    "            continue\n",
    "        \n",
    "        pred = False if \"unsame\" in pred else True\n",
    "        pred_list.append(pred)\n",
    "        gt_list.append(gt)\n",
    "    \n",
    "    assert len(gt_list) == len(pred_list)\n",
    "    score = sum(1 for x,y in zip(pred_list, gt_list) if x == y) / len(pred_list)\n",
    "    return score\n",
    "\n",
    "\n",
    "def evaluate_permutate(model_name, log_dir=\"logs/permutated_list\"):\n",
    "    \"\"\"\n",
    "    Evaluate the permutated task\n",
    "    Metric: Accuracy\n",
    "    \"\"\"\n",
    "    if not os.path.exists(log_dir):\n",
    "        raise ValueError(f\"logs_dir {log_dir} is not correct\")\n",
    "    gt_list = []\n",
    "    pred_list = []\n",
    "    pred_results = read_json(f\"{log_dir}/{model_name}.json\")\n",
    "    for pred in pred_results:\n",
    "        gt = True\n",
    "        pred = parse_response(pred['json_results'])\n",
    "        pred = pred.lower()\n",
    "\n",
    "        # if failed to parse. we treat the pred as False and gt as True\n",
    "        # this is a special case for the model that cannot parse the response\n",
    "        if pred == \"\":\n",
    "            pred_list.append(False)\n",
    "            gt_list.append(True)\n",
    "            continue\n",
    "\n",
    "        pred = False if \"unsame\" in pred else True\n",
    "        pred_list.append(pred)\n",
    "        gt_list.append(gt)\n",
    "    \n",
    "    assert len(gt_list) == len(pred_list)\n",
    "    score = sum(1 for x,y in zip(pred_list, gt_list) if x == y) / len(pred_list)\n",
    "    return score\n",
    "\n",
    "\n",
    "def evaluate_murcko(model_name, log_dir=\"logs/murcko_scaffold\"):\n",
    "    \"\"\"\n",
    "    Evaluate the murcko scaffold task\n",
    "    Metric: Tanimoto molecule similarity\n",
    "    \"\"\"\n",
    "    if not os.path.exists(log_dir):\n",
    "        raise ValueError(f\"logs_dir {log_dir} is not correct\")\n",
    "    gt_list = []\n",
    "    pred_list = []\n",
    "    pred_results = read_json(f\"{log_dir}/{model_name}.json\")\n",
    "    for pred in pred_results:\n",
    "        gt = pred[gt_key_dict['murcko']]\n",
    "        pred = parse_response(pred['json_results'])\n",
    "        pred_list.append(pred)\n",
    "        gt_list.append(gt)\n",
    "    \n",
    "    same_count, mean_scaffold_score = scaffold_consistency(pred_list, gt_list)\n",
    "    return mean_scaffold_score\n",
    "\n",
    "\n",
    "def evaluate_ring_system(model_name, log_dir=\"logs/ring_system_scaffold\"):\n",
    "    \"\"\"\n",
    "    Evaluate the ring system task\n",
    "    Metric: Accuracy\n",
    "    \"\"\"\n",
    "    if not os.path.exists(log_dir):\n",
    "        raise ValueError(f\"logs_dir {log_dir} is not correct\")\n",
    "    gt_list = []\n",
    "    pred_list = []\n",
    "    pred_results = read_json(f\"{log_dir}/{model_name}.json\")\n",
    "    for pred in pred_results:\n",
    "        gt = pred['gt']\n",
    "        gt = gt.lower() # yes/no\n",
    "        pred = parse_response(pred['json_results'])\n",
    "        \n",
    "        # If failed to parse, we treat the pred as False and gt as True\n",
    "        # this is a special case for the model that cannot parse the response\n",
    "        if pred == \"\":\n",
    "            pred_list.append(False)\n",
    "            gt_list.append(True)\n",
    "            continue\n",
    "        \n",
    "        # detailed parsing to determine the pred as yes/no\n",
    "        pred = pred.lower()\n",
    "        if \"yes\" in pred:\n",
    "            pred = True\n",
    "        elif \"no\" in pred:\n",
    "            pred = False\n",
    "        else:\n",
    "            pred = False\n",
    "        pred_list.append(pred)\n",
    "        gt_list.append(gt)\n",
    "    \n",
    "    assert len(gt_list) == len(pred_list)\n",
    "    score = sum(1 for x,y in zip(pred_list, gt_list) if x == y) / len(pred_list)\n",
    "    return score\n",
    "\n",
    "\n",
    "print('fg_samples: ', evaluate_fg_sample('ether0', '../mol_und_edit/logs/fg_samples'))\n",
    "print('ring_count: ', evaluate_ring_count('ether0', '../mol_und_edit/logs/ring_count'))\n",
    "\n",
    "mutated_score = evaluate_mutate('ether0', '../mol_und_edit/logs/mutated_list')\n",
    "permutated_score = evaluate_permutate('ether0', '../mol_und_edit/logs/permutated_list')\n",
    "eq_score = (mutated_score + permutated_score) / 2\n",
    "print('eq_score: ', eq_score)\n",
    "\n",
    "print('murcko: ', evaluate_murcko('ether0', '../mol_und_edit/logs/murcko_scaffold'))\n",
    "print('ring_system: ', evaluate_ring_system('ether0', '../mol_und_edit/logs/ring_system_scaffold'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mol-Edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval.eval_moledit import check_edit_add_valid, check_edit_del_valid, check_edit_sub_valid\n",
    "def evaluate_moledit_score(task, model_name, log_dir=\"logs/mol_edit\"):\n",
    "    \"\"\"\n",
    "    Evaluate the moledit score\n",
    "    metric: accuracy\n",
    "    Args:\n",
    "        task: the task name\n",
    "        model_name: the model name\n",
    "        log_dir: the directory of the logs\n",
    "    Returns:\n",
    "        the score\n",
    "    \"\"\"\n",
    "    assert task in ['add', 'delete', 'sub'], \"task must be one of add, delete, sub\"\n",
    "    if not os.path.exists(os.path.join(log_dir, task)):\n",
    "        raise ValueError(f\"logs_dir {log_dir} is not correct\")\n",
    "    samples = read_json(f\"{log_dir}/{task}/{model_name}.json\")\n",
    "\n",
    "    invalid_number = 0\n",
    "    pred_list, src_list = list(), list()\n",
    "    group_a_list, group_b_list = list(), list()\n",
    "    for sample in samples:\n",
    "        if task == 'add':\n",
    "            group_a = sample['added_group']\n",
    "            group_b = None\n",
    "        elif task == 'delete':\n",
    "            group_a = sample['removed_group']\n",
    "            group_b = None\n",
    "        elif task == 'sub':\n",
    "            group_a = sample['added_group']\n",
    "            group_b = sample['removed_group']\n",
    "        \n",
    "        src = sample['molecule']\n",
    "        # try to extract predicted-smiles\n",
    "        json_results = sample['json_results']\n",
    "        extracted = parse_response(json_results)\n",
    "        if extracted == \"\":\n",
    "            invalid_number += 1\n",
    "            continue\n",
    "        \n",
    "        pred_list.append(extracted)\n",
    "        src_list.append(src)\n",
    "        group_a_list.append(group_a)\n",
    "        if group_b is not None:\n",
    "            group_b_list.append(group_b)\n",
    "    \n",
    "    assert len(src_list) == len(pred_list) == len(group_a_list)\n",
    "    # calculate the score\n",
    "    correct_num = 0\n",
    "    for i in range(len(src_list)):\n",
    "        if task in ['add']:\n",
    "            if check_edit_add_valid(src=src_list[i], tgt=pred_list[i], group=group_a_list[i]):\n",
    "                correct_num += 1\n",
    "        if task in ['delete']:\n",
    "            if check_edit_del_valid(src=src_list[i], tgt=pred_list[i], group=group_a_list[i]):\n",
    "                correct_num += 1\n",
    "        if task == 'sub':\n",
    "            if check_edit_sub_valid(src=src_list[i], tgt=pred_list[i], remove_group=group_b_list[i], add_group=group_a_list[i]):\n",
    "                correct_num += 1\n",
    "    print(f\"invalid number: {invalid_number}\")\n",
    "    return correct_num / len(src_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "evaluate add\n",
      "添加amide失败: 目标分子中amide数量为1, 源分子中amide数量为1\n",
      "invalid number: 2\n",
      "add_score:  0.9444444444444444\n",
      "#########################\n",
      "\n",
      "#########################\n",
      "evaluate delete\n",
      "invalid number: 3\n",
      "delete_score:  0.7647058823529411\n",
      "#########################\n",
      "\n",
      "#########################\n",
      "evaluate sub\n",
      "invalid number: 2\n",
      "sub_score:  0.7758620689655172\n",
      "#########################\n",
      "\n",
      "weighted_score:  0.8073473067387875\n"
     ]
    }
   ],
   "source": [
    "print(\"#########################\")\n",
    "print(\"evaluate add\")\n",
    "add_score = evaluate_moledit_score(\"add\", MODEL_NAME, log_dir=\"../mol_und_edit/logs\")\n",
    "print(\"add_score: \", add_score)\n",
    "print(\"#########################\\n\")\n",
    "\n",
    "print(\"#########################\")\n",
    "print(\"evaluate delete\")\n",
    "delete_score = evaluate_moledit_score(\"delete\", MODEL_NAME, log_dir=\"../mol_und_edit/logs\")\n",
    "print(\"delete_score: \", delete_score)\n",
    "print(\"#########################\\n\")\n",
    "\n",
    "print(\"#########################\")\n",
    "print(\"evaluate sub\")\n",
    "sub_score = evaluate_moledit_score(\"sub\", MODEL_NAME, log_dir=\"../mol_und_edit/logs\")\n",
    "print(\"sub_score: \", sub_score)\n",
    "print(\"#########################\\n\")\n",
    "\n",
    "weighted_score = add_score * 0.2 + delete_score * 0.2 + sub_score * 0.6\n",
    "print(\"weighted_score: \", weighted_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mol-opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Optional, Literal\n",
    "import json\n",
    "\n",
    "def tranform_str_to_json(str_input):\n",
    "    ## 假如LLM输出的是类似json的字符串, 我需要设定一个逻辑, 把字符串重新转换成json\n",
    "    ## o1-mini的感觉, 是要移除字符串里面的\\n，并且把所有的\\\"都改成 \"\n",
    "    if \"```json\\n\" in str_input:\n",
    "        str_input = str_input.split(\"```json\\n\")[1]\n",
    "        str_input = str_input.replace(\"\\n```\", '')\n",
    "    \n",
    "    unescaped_str = str_input.replace('\\n    ', '').replace('\\n', '').replace('\\\"', '\"')\n",
    "    try:\n",
    "        json_obj = json.loads(unescaped_str)\n",
    "        return json_obj\n",
    "    except json.JSONDecodeError as e:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def _validate_format(value: str, format: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    验证值的类型是否符合指定的 format。\n",
    "\n",
    "    Args:\n",
    "        value (str): 提取的原始值（字符串形式）。\n",
    "        format (str): 期望的类型（\"str\"、\"int\"、\"float\"、\"bool\"）。\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: 转换后的值（字符串形式），如果类型不匹配则返回 None。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if format == \"int\":\n",
    "            int(value)\n",
    "            return value\n",
    "        elif format == \"float\":\n",
    "            float(value)\n",
    "            return value\n",
    "        elif format == \"bool\":\n",
    "            if value.lower() in (\"true\", \"false\"):\n",
    "                return value.lower()\n",
    "            return None\n",
    "        elif format == \"str\":\n",
    "            return value\n",
    "        return None\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_raw_response(\n",
    "    raw_response: str,\n",
    "    field: str,\n",
    "    format: Literal[\"str\", \"int\", \"float\", \"bool\"] = \"str\"\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    从 JSON 格式字符串中提取指定字段的值，忽略开头的 <think>...</think> 部分，\n",
    "    并根据 format 参数验证值的类型。\n",
    "\n",
    "    Args:\n",
    "        raw_response (str): 包含 JSON 数据的字符串，可能以 <think>...</think> 开头。\n",
    "        field (str): 要提取的字段名（如 \"count\"）。\n",
    "        format (Literal[\"str\", \"int\", \"float\", \"bool\"]): 期望的返回值类型，默认为 \"str\"。\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: 字段的值（字符串形式），如果未找到或类型不匹配则返回 None。\n",
    "    \"\"\"\n",
    "    # 1. 移除 <think>...</think> 部分（如果有）\n",
    "    cleaned_response = re.sub(r'<think>.*?</think>', '', raw_response, flags=re.DOTALL)\n",
    "\n",
    "    # 2. 尝试匹配带引号的字符串值（如 \"count\": \"2\"）\n",
    "    quoted_pattern = rf'\"{field}\":\\s*\"([^\"]+)\"'\n",
    "    match = re.search(quoted_pattern, cleaned_response)\n",
    "    if match:\n",
    "        value = match.group(1)\n",
    "        if format == \"str\":\n",
    "            return value\n",
    "        return _validate_format(value, format)\n",
    "\n",
    "    # 3. 尝试匹配不带引号的值（如 \"count\": 2, \"active\": true）\n",
    "    unquoted_pattern = rf'\"{field}\":\\s*([^,}}\\s]+)'\n",
    "    match = re.search(unquoted_pattern, cleaned_response)\n",
    "    if match:\n",
    "        value = match.group(1).strip()\n",
    "        return _validate_format(value, format)\n",
    "\n",
    "    # 4. 未找到字段\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from eval.eval_metric import mol_opt_evaluater\n",
    "from eval.eval_metric import calculate_solubility, compute_statistics\n",
    "\n",
    "prop_dict = dict(\n",
    "    logp=\"logp\", \n",
    "    solubility=\"solubility\", \n",
    "    qed=\"qed\", \n",
    "    drd=\"drd2\", \n",
    "    gsk=\"gsk3b\", \n",
    "    jnk=\"jnk3\"\n",
    ")\n",
    "\n",
    "\n",
    "def evaluate_molopt_score(task:str, model_name:str, log_dir:str=\"logs/mol_opt\"):\n",
    "    if not os.path.exists(os.path.join(log_dir, task)):\n",
    "        raise ValueError(f\"logs_dir {log_dir} is not correct\")\n",
    "    prop_evaluater = mol_opt_evaluater(prop=prop_dict[task])\n",
    "    samples = read_json(f\"{log_dir}/{task}/{model_name}.json\")\n",
    "    invalid_number = 0\n",
    "    pred_list, src_list = list(), list()\n",
    "    for sample in samples:\n",
    "        src = sample['src_smiles']\n",
    "\n",
    "        # try to extract predicted-smiles\n",
    "        json_results = sample['json_results']\n",
    "        extracted = parse_response(json_results)\n",
    "        if extracted == \"\":\n",
    "            # print(f\"cannot parse from sample {sample['id']}\")\n",
    "            invalid_number += 1\n",
    "            continue\n",
    "        \n",
    "        pred_list.append(extracted)\n",
    "        src_list.append(src)\n",
    "    \n",
    "    assert len(src_list) == len(pred_list)\n",
    "    # calculate the score\n",
    "    improve_scores = prop_evaluater.property_improvement(\n",
    "        src_mol_list=src_list, \n",
    "        tgt_mol_list=pred_list, \n",
    "        total_num=len(samples)\n",
    "    )\n",
    "\n",
    "    print(f\"invalid number: {invalid_number}\")\n",
    "    print(f\"improvement mean: {round(improve_scores.get('mean'), 2)}\")\n",
    "    print(f\"improvement std: {improve_scores.get('variance')}\")\n",
    "    print(f\"success_rate: {round(improve_scores.get('success_rate'), 2)}\")\n",
    "\n",
    "def evaluate_solubility_score(model_name, log_dir=\"logs/mol_opt/\"):\n",
    "    if not os.path.exists(os.path.join(log_dir, \"solubility\")):\n",
    "        raise ValueError(f\"logs_dir {log_dir} is not correct\")\n",
    "    samples = read_json(f\"{log_dir}/solubility/{model_name}.json\")\n",
    "    src_list = []\n",
    "    tgt_list = []\n",
    "    key = 'Final Target Molecule'\n",
    "    invalid_number = 0\n",
    "    for x in samples:\n",
    "        json_results = x['json_results']\n",
    "        src_smiles = x['src_smiles']\n",
    "        if type(json_results) is dict and key in json_results:\n",
    "            extracted = json_results[key]\n",
    "        elif type(json_results) is str:\n",
    "            # 如果直接是个SMILES字符串, 则直接添加\n",
    "            if is_valid_smiles(json_results, strict=False):\n",
    "                extracted = json_results\n",
    "            else:\n",
    "                extracted = parse_raw_response(json_results, field=key)\n",
    "                if extracted == \"\" or extracted is None:\n",
    "                    invalid_number += 1\n",
    "                    continue\n",
    "        else:\n",
    "            # print(f\"cannot parse from sample {x['id']}\")\n",
    "            invalid_number += 1\n",
    "            continue\n",
    "        src_list.append(src_smiles)\n",
    "        tgt_list.append(extracted)\n",
    "\n",
    "\n",
    "    gains = []\n",
    "    for s,t in zip(src_list, tgt_list):\n",
    "        if not is_valid_smiles(s) or not is_valid_smiles(t):\n",
    "            gains.append(0.)\n",
    "        else:\n",
    "            gains.append(calculate_solubility(t) - calculate_solubility(s))\n",
    "\n",
    "    stats = compute_statistics(gains, 'solubility', skew=True)\n",
    "    print(f\"invalid number: {invalid_number}\")\n",
    "    print(f\"improvement mean: {round(stats.get('mean'), 2)}\")\n",
    "    print(f\"improvement std: {stats.get('variance')}\")\n",
    "    print(f\"success_rate: {round(stats.get('success_rate'), 2)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate logp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid number: 1\n",
      "improvement mean: 0.37\n",
      "improvement std: 1.159692890137555\n",
      "success_rate: 0.68\n",
      "None\n",
      "\n",
      "Evaluate solubility\n",
      "invalid number: 93\n",
      "improvement mean: 0.0\n",
      "improvement std: 0.0\n",
      "success_rate: 0.0\n",
      "None\n",
      "\n",
      "Evaluate qed\n",
      "invalid number: 0\n",
      "improvement mean: -0.37\n",
      "improvement std: 0.027038276905880027\n",
      "success_rate: 0.05\n",
      "None\n",
      "\n",
      "Evaluate drd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid number: 0\n",
      "improvement mean: -0.14\n",
      "improvement std: 0.025330559277142978\n",
      "success_rate: 0.04\n",
      "None\n",
      "\n",
      "Evaluate gsk\n",
      "invalid number: 0\n",
      "improvement mean: 0.0\n",
      "improvement std: 0.0\n",
      "success_rate: 0.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluate logp\")\n",
    "print(evaluate_molopt_score(\"logp\", MODEL_NAME, log_dir=\"../mol_opt/logs\"))\n",
    "\n",
    "print(\"\\nEvaluate solubility\")\n",
    "print(evaluate_solubility_score(MODEL_NAME, log_dir=\"../mol_opt/logs\"))\n",
    "\n",
    "print(\"\\nEvaluate qed\")\n",
    "print(evaluate_molopt_score(\"qed\", MODEL_NAME, log_dir=\"../mol_opt/logs\"))\n",
    "\n",
    "print(\"\\nEvaluate drd\")\n",
    "print(evaluate_molopt_score(\"drd\", MODEL_NAME, log_dir=\"../mol_opt/logs\"))\n",
    "\n",
    "print(\"\\nEvaluate gsk\")\n",
    "print(evaluate_molopt_score(\"gsk\", MODEL_NAME, log_dir=\"../mol_opt/logs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: for jnk evaluation, the oracle pkl is old. So please change to some old version.\n",
    "\n",
    "print(\"\\nEvaluate jnk\")\n",
    "print(evaluate_molopt_score(\"jnk\", MODEL_NAME, log_dir=\"../mol_opt/logs\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
