{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from eval.eval_molund import tranform_str_to_json, check_string_type\n",
    "from eval.eval_molund import eval_molund_from_list\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claude3.7 fg_samples\n",
      "claude3.7 fg_samples {'score': 0.21, 'fg_samples-valid-rate': 1.0}\n",
      "claude3.7 murcko\n",
      "claude3.7 murcko {'score': 0.3995904332070048, 'murcko-valid-rate': 0.95}\n",
      "claude3.7 ring_count\n",
      "claude3.7 ring_count {'score': 1.6, 'ring_count-valid-rate': 1.0}\n",
      "claude3.7 ring_system\n",
      "claude3.7 ring_system {'score': 0.7948717948717948, 'ring_system-valid-rate': 0.975}\n",
      "claude3.7 mutated\n",
      "claude3.7 mutated {'score': 0.7209302325581395, 'mutated-valid-rate': 0.86}\n",
      "claude3.7 permutated\n",
      "claude3.7 permutated {'score': 0.9591836734693877, 'permutated-valid-rate': 0.98}\n",
      "eval_score_claude3.7 {'fg_samples': {'score': 0.21, 'fg_samples-valid-rate': 1.0}, 'murcko': {'score': 0.3995904332070048, 'murcko-valid-rate': 0.95}, 'ring_count': {'score': 1.6, 'ring_count-valid-rate': 1.0}, 'ring_system': {'score': 0.7948717948717948, 'ring_system-valid-rate': 0.975}, 'mutated': {'score': 0.7209302325581395, 'mutated-valid-rate': 0.86}, 'permutated': {'score': 0.9591836734693877, 'permutated-valid-rate': 0.98}}\n"
     ]
    }
   ],
   "source": [
    "def evaluate_molund_score(model_name):\n",
    "    task_dict = dict(\n",
    "        fg_samples=\"fg_samples\", murcko='murcko_scaffold', ring_count='ring_count',\n",
    "        ring_system='ring_system_scaffold', mutated='mutated', permutated='permutated'\n",
    "    )\n",
    "    pred_key_dict = dict(\n",
    "        fg_samples=\"count\", murcko='Output Scaffold', ring_count='count',\n",
    "        ring_system='output', mutated='output', permutated='output'\n",
    "    )\n",
    "    gt_key_dict = dict(\n",
    "        fg_samples=\"fg_num\", murcko='largest_scaffold', ring_count='count',\n",
    "        ring_system='', mutated='', permutated=''\n",
    "    )\n",
    "    result_dict = dict()\n",
    "    \n",
    "    for task in task_dict.keys():\n",
    "        print(model_name, task)\n",
    "        if 'llama' not in model_name:\n",
    "            file_name = f\"logs/{task_dict[task]}/{model_name}.json\"\n",
    "            \n",
    "        pred_results = json.load(open(file_name, \"r\"))\n",
    "        invalid_number = 0\n",
    "        \n",
    "        pred_list, gt_list = list(), list()\n",
    "        for pred in pred_results:  \n",
    "            if type(pred['json_results']) is str:\n",
    "                pred_json = tranform_str_to_json(str_input=pred['json_results'])\n",
    "                # if model_name == 'gemini': pred_json = pred_json[0]\n",
    "                if pred_json == None:\n",
    "                    invalid_number += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    if pred_key_dict[task] not in pred_json.keys():\n",
    "                        invalid_number += 1; continue\n",
    "                    if pred_json[pred_key_dict[task]] == \"\": \n",
    "                        invalid_number += 1; continue\n",
    "                    if task in [\"ring_count\", \"fg_samples\"]:\n",
    "                        if check_string_type(pred_json[pred_key_dict[task]]) == \"string\":\n",
    "                            invalid_number += 1; continue;\n",
    "                    pred_list.append(pred_json[pred_key_dict[task]])\n",
    "                    if gt_key_dict[task] != \"\":\n",
    "                        gt_list.append(pred[gt_key_dict[task]])\n",
    "            else:\n",
    "                if pred_key_dict[task] not in pred['json_results'].keys():\n",
    "                    invalid_number += 1; continue\n",
    "                if pred['json_results'][pred_key_dict[task]] == \"\": \n",
    "                    invalid_number += 1; continue\n",
    "                if task in [\"ring_count\", \"fg_samples\"]:\n",
    "                    if check_string_type(pred['json_results'][pred_key_dict[task]]) == \"string\":\n",
    "                            invalid_number += 1; continue;\n",
    "                pred_list.append(pred['json_results'][pred_key_dict[task]])\n",
    "                if gt_key_dict[task] != \"\":\n",
    "                    gt_list.append(pred[gt_key_dict[task]])\n",
    "        \n",
    "        assert len(pred_results) == invalid_number+len(pred_list)\n",
    "        result_dict[task] = eval_molund_from_list(gt_list=gt_list, pred_list=pred_list, total_number=len(pred_results), task=task)\n",
    "        print(model_name, task, result_dict[task])\n",
    "    \n",
    "    print(f\"eval_score_{model_name}\", result_dict)\n",
    "    # json.dump(result_dict, open(f\"logs/eval_score_{model_name}.json\", \"w\"), indent=4)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_list = ['claude3.7']\n",
    "    for model_name in model_list:\n",
    "        evaluate_molund_score(model_name=model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mol_opt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
